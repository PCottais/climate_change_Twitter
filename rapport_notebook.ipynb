{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d3f697",
   "metadata": {},
   "source": [
    "# Projet de _Computer science for big data_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00afb5",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a422a67",
   "metadata": {},
   "source": [
    "### Présentation des données <a id=\"data_pres\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88817b",
   "metadata": {},
   "source": [
    "Les données exploitées sont en partie disponibles sur le site Kaggle. \n",
    "Le jeu de données est uniquement composé de tweets portant sur le réchauffement climatique et déjà classifiés en quatre groupes possibles :\n",
    "* Ne croit pas au réchauffement climatique {sentiment: -1}\n",
    "* Sans opinion {sentiment: 0}\n",
    "* Croit au réchauffement climatique {sentiment: 1}\n",
    "* Relaie des faits d'actualité à propos du réchauffement climatique {sentiment: 2}\n",
    "\n",
    "Cependant, une manipulation spécifique est nécessaire afin de récolter plus d'information sur ces tweets, au moyen d'une fusion de deux sources différentes.\n",
    "Le site \\[nom du site\\] suivant permet de récupérer une grande quantité d'information supplémentaires sur ces tweets.\n",
    "La fusion est faite selon l'ID commun à ces deux sources différentes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6012c",
   "metadata": {},
   "source": [
    "### Problématique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079615a",
   "metadata": {},
   "source": [
    "L'objectif de cette étude est de comprendre la classification qui est déjà faite sur le contenu de chaque tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fef90",
   "metadata": {},
   "source": [
    "### Outils utilisés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f402ae",
   "metadata": {},
   "source": [
    "En plus la jointure de deux sources de données, abordée dans la partie [Présentation des données](#data_pres), d'autres outils sont utilisés pour le traitement des données.\n",
    "1. Pour la construction du jeu de données, l'API Hydrator permet de télécharger autant de tweets (et tout leur contenu) que l'on souhaite (palliant ainsi le problème de la limitation de 20 tweets par jour depuis Twitter directement).\n",
    "2. Dans un premier temps, il faut installer MongoDB sur Windows et créer une base de données vierge par l'intermédiaire de l'invite de commandes Windows. Ensuite, les données sont importées dans cette nouvelle base sous forme de collection à partir d'un fichier au format JSON.\n",
    "3. Cette base et collection crées, il faut ensuite s'y connecter depuis un script Python à l'aide des fonctions du module ```pymongo```.\n",
    "4. La première partie de l'étude est une analyse exploratoire du contenu des tweets afin d'y désceller des caractéristiques pouvant être déterminantes dans la classification. Pour cette partie, différents modules sont utilisés pour valoriser  certains traitements de données et les résultats présentés dans la partie [Analyse exploratoire avec MongoDB](#partie1).\n",
    "5. La seconde partie est dédiée à l'utilisation de Spark par l'intermédiare du module ```pyspark```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
